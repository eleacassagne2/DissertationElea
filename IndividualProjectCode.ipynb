{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qknZmnQ_3cfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHBmSO2LlwJe",
        "outputId": "3e7d08c5-76b3-4dd3-b77e-bf886717e079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Collecting pretty-errors==1.2.25 (from torchmetrics)\n",
            "  Downloading pretty_errors-1.2.25-py3-none-any.whl (17 kB)\n",
            "Collecting colorama (from pretty-errors==1.2.25->torchmetrics)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, colorama, pretty-errors, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed colorama-0.4.6 lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pretty-errors-1.2.25 torchmetrics-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kymatio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9_VWcMT3eY0",
        "outputId": "6e1ece2a-8509-4cba-d609-0042b47f4b6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kymatio\n",
            "  Downloading kymatio-0.3.0-py3-none-any.whl (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/87.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from kymatio) (1.4.4)\n",
            "Collecting configparser (from kymatio)\n",
            "  Downloading configparser-7.0.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kymatio) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kymatio) (24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kymatio) (1.11.4)\n",
            "Installing collected packages: configparser, kymatio\n",
            "Successfully installed configparser-7.0.0 kymatio-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from os import path\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from typing import Sequence\n",
        "from torchvision.transforms import functional as F\n",
        "import numbers\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import torchmetrics as TM\n",
        "from kymatio.torch import Scattering2D\n",
        "\n",
        "# Convert a pytorch tensor into a PIL image\n",
        "t2img = T.ToPILImage()\n",
        "# Convert a PIL image into a pytorch tensor\n",
        "img2t = T.ToTensor()\n",
        "\n",
        "# Set the working (writable) directory.\n",
        "working_dir = \"/kaggle/working/\""
      ],
      "metadata": {
        "id": "Xz6zyKpy3iPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_checkpoint(model, cp_name):\n",
        "    torch.save(model.state_dict(), os.path.join(working_dir, cp_name))\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "# Load model from saved checkpoint\n",
        "def load_model_from_checkpoint(model, ckp_path):\n",
        "    return model.load_state_dict(\n",
        "        torch.load(\n",
        "            ckp_path,\n",
        "            map_location=get_device(),\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Send the Tensor or Model (input argument x) to the right device\n",
        "# for this notebook. i.e. if GPU is enabled, then send to GPU/CUDA\n",
        "# otherwise send to CPU.\n",
        "def to_device(x):\n",
        "    if torch.cuda.is_available():\n",
        "        return x.cuda()\n",
        "    else:\n",
        "        return x.cpu()\n",
        "\n",
        "def get_model_parameters(m):\n",
        "    total_params = sum(\n",
        "        param.numel() for param in m.parameters()\n",
        "    )\n",
        "    return total_params\n",
        "\n",
        "def print_model_parameters(m):\n",
        "    num_model_parameters = get_model_parameters(m)\n",
        "    print(f\"The Model has {num_model_parameters/1e6:.2f}M parameters\")\n",
        "# end if\n",
        "\n",
        "def close_figures():\n",
        "    while len(plt.get_fignums()) > 0:\n",
        "        plt.close()\n",
        "    # end while\n",
        "# end def\n",
        "\n",
        "# Validation: Check if CUDA is available\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "nKgvFcMb3lTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oxford IIIT Pets Segmentation dataset loaded via torchvision.\n",
        "pets_path_train = os.path.join(working_dir, 'OxfordPets', 'train')\n",
        "pets_path_test = os.path.join(working_dir, 'OxfordPets', 'test')\n",
        "pets_train_orig = torchvision.datasets.OxfordIIITPet(root=pets_path_train, split=\"trainval\", target_types=\"segmentation\", download=True)\n",
        "pets_test_orig = torchvision.datasets.OxfordIIITPet(root=pets_path_test, split=\"test\", target_types=\"segmentation\", download=True)"
      ],
      "metadata": {
        "id": "KV5rVtnX3o8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pets_train_orig, pets_test_orig"
      ],
      "metadata": {
        "id": "msleemlT3r0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_pets_input, train_pets_target) = pets_train_orig[0]"
      ],
      "metadata": {
        "id": "fQedUz8k3vDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spot check an input image.\n",
        "train_pets_input"
      ],
      "metadata": {
        "id": "6nd9X2S03vGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import IntEnum\n",
        "class TrimapClasses(IntEnum):\n",
        "    PET = 0\n",
        "    BACKGROUND = 1\n",
        "    BORDER = 2"
      ],
      "metadata": {
        "id": "5Aq4yP8m3vI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert a float trimap ({1, 2, 3} / 255.0) into a float tensor with\n",
        "# pixel values in the range 0.0 to 1.0 so that the border pixels\n",
        "# can be properly displayed.\n",
        "def trimap2f(trimap):\n",
        "    return (img2t(trimap) * 255.0 - 1) / 2"
      ],
      "metadata": {
        "id": "xsAcAkuR3vLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spot check a segmentation mask image after post-processing it\n",
        "# via trimap2f().\n",
        "t2img(trimap2f(train_pets_target))"
      ],
      "metadata": {
        "id": "h8K7jN7i3vOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple torchvision compatible transform to send an input tensor\n",
        "# to a pre-specified device.\n",
        "class ToDevice(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Sends the input object to the device specified in the\n",
        "    object's constructor by calling .to(device) on the object.\n",
        "    \"\"\"\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, img):\n",
        "        return img.to(self.device)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"{self.__class__.__name__}(device={device})\""
      ],
      "metadata": {
        "id": "i9aExhhY3vPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset wrapper that allows us to perform custom image augmentations\n",
        "# on both the target and label (segmentation mask) images.\n",
        "#\n",
        "# These custom image augmentations are needed since we want to perform\n",
        "# transforms such as:\n",
        "# 1. Random horizontal flip\n",
        "# 2. Image resize\n",
        "#\n",
        "# and these operations need to be applied consistently to both the input\n",
        "# image as well as the segmentation mask.\n",
        "class OxfordIIITPetsAugmented(torchvision.datasets.OxfordIIITPet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        split: str,\n",
        "        target_types=\"segmentation\",\n",
        "        download=False,\n",
        "        pre_transform=None,\n",
        "        post_transform=None,\n",
        "        pre_target_transform=None,\n",
        "        post_target_transform=None,\n",
        "        common_transform=None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            root=root,\n",
        "            split=split,\n",
        "            target_types=target_types,\n",
        "            download=download,\n",
        "            transform=pre_transform,\n",
        "            target_transform=pre_target_transform,\n",
        "        )\n",
        "        self.post_transform = post_transform\n",
        "        self.post_target_transform = post_target_transform\n",
        "        self.common_transform = common_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return super().__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        (input, target) = super().__getitem__(idx)\n",
        "\n",
        "        # Common transforms are performed on both the input and the labels\n",
        "        # by creating a 4 channel image and running the transform on both.\n",
        "        # Then the segmentation mask (4th channel) is separated out.\n",
        "        if self.common_transform is not None:\n",
        "            both = torch.cat([input, target], dim=0)\n",
        "            both = self.common_transform(both)\n",
        "            (input, target) = torch.split(both, 3, dim=0)\n",
        "        # end if\n",
        "\n",
        "        if self.post_transform is not None:\n",
        "            input = self.post_transform(input)\n",
        "        if self.post_target_transform is not None:\n",
        "            target = self.post_target_transform(target)\n",
        "\n",
        "        return (input, target)"
      ],
      "metadata": {
        "id": "bxEaODoI3vSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor for a segmentation trimap.\n",
        "# Input: Float tensor with values in [0.0 .. 1.0]\n",
        "# Output: Long tensor with values in {0, 1, 2}\n",
        "def tensor_trimap(t):\n",
        "    x = t * 255\n",
        "    x = x.to(torch.long)\n",
        "    x = x - 1\n",
        "    return x\n",
        "\n",
        "def args_to_dict(**kwargs):\n",
        "    return kwargs\n",
        "\n",
        "transform_dict = args_to_dict(\n",
        "    pre_transform=T.ToTensor(),\n",
        "    pre_target_transform=T.ToTensor(),\n",
        "    common_transform=T.Compose([\n",
        "        ToDevice(get_device()),\n",
        "        T.Resize((128, 128), interpolation=T.InterpolationMode.NEAREST),\n",
        "        # Random Horizontal Flip as data augmentation.\n",
        "        T.RandomHorizontalFlip(p=0.5),\n",
        "    ]),\n",
        "    post_transform=T.Compose([\n",
        "        # Color Jitter as data augmentation.\n",
        "        T.ColorJitter(contrast=0.3),\n",
        "    ]),\n",
        "    post_target_transform=T.Compose([\n",
        "        T.Lambda(tensor_trimap),\n",
        "    ]),\n",
        ")\n",
        "\n",
        "# Create the train and test instances of the data loader for the\n",
        "# Oxford IIIT Pets dataset with random augmentations applied.\n",
        "# The images are resized to 128x128 squares, so the aspect ratio\n",
        "# will be chaged. We use the nearest neighbour resizing algorithm\n",
        "# to avoid disturbing the pixel values in the provided segmentation\n",
        "# mask.\n",
        "pets_train = OxfordIIITPetsAugmented(\n",
        "    root=pets_path_train,\n",
        "    split=\"trainval\",\n",
        "    target_types=\"segmentation\",\n",
        "    download=False,\n",
        "    **transform_dict,\n",
        ")\n",
        "pets_test = OxfordIIITPetsAugmented(\n",
        "    root=pets_path_test,\n",
        "    split=\"test\",\n",
        "    target_types=\"segmentation\",\n",
        "    download=False,\n",
        "    **transform_dict,\n",
        ")\n",
        "\n",
        "pets_train_loader = torch.utils.data.DataLoader(\n",
        "    pets_train,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        ")\n",
        "pets_test_loader = torch.utils.data.DataLoader(\n",
        "    pets_test,\n",
        "    batch_size=21,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "(train_pets_inputs, train_pets_targets) = next(iter(pets_train_loader))\n",
        "(test_pets_inputs, test_pets_targets) = next(iter(pets_test_loader))\n",
        "train_pets_inputs.shape, train_pets_targets.shape"
      ],
      "metadata": {
        "id": "ES0NCX7h3vVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's inspect some of the images.\n",
        "pets_input_grid = torchvision.utils.make_grid(train_pets_inputs, nrow=8)\n",
        "t2img(pets_input_grid)"
      ],
      "metadata": {
        "id": "TAXYypHu3vZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's inspect the segmentation masks corresponding to the images above.\n",
        "#\n",
        "# When plotting the segmentation mask, we want to convert the tensor\n",
        "# into a float tensor with values in the range [0.0 to 1.0]. However, the\n",
        "# mask tensor has the values (0, 1, 2), so we divide by 2.0 to normalize.\n",
        "pets_targets_grid = torchvision.utils.make_grid(train_pets_targets / 2.0, nrow=8)\n",
        "t2img(pets_targets_grid)"
      ],
      "metadata": {
        "id": "aNzE9biK3vce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#                  N  C  H\n",
        "# The printed row is the W dimension.\n",
        "train_pets_targets[3][0][4]"
      ],
      "metadata": {
        "id": "7dMQbJdp3ve6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition. We use a SegNet-Basic model with some minor tweaks.\n",
        "# Our input images are 128x128.\n",
        "class DownConv2(nn.Module):\n",
        "    def __init__(self, chin, chout, kernel_size):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mp = nn.MaxPool2d(kernel_size=2, return_indices=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.seq(x)\n",
        "        pool_shape = y.shape\n",
        "        y, indices = self.mp(y)\n",
        "        return y, indices, pool_shape\n",
        "\n",
        "class DownConv3(nn.Module):\n",
        "    def __init__(self, chin, chout, kernel_size):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mp = nn.MaxPool2d(kernel_size=2, return_indices=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.seq(x)\n",
        "        pool_shape = y.shape\n",
        "        y, indices = self.mp(y)\n",
        "        return y, indices, pool_shape\n",
        "\n",
        "class UpConv2(nn.Module):\n",
        "    def __init__(self, chin, chout, kernel_size):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chin),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mup = nn.MaxUnpool2d(kernel_size=2)\n",
        "\n",
        "    def forward(self, x, indices, output_size):\n",
        "        y = self.mup(x, indices, output_size=output_size)\n",
        "        y = self.seq(y)\n",
        "        return y\n",
        "\n",
        "class UpConv3(nn.Module):\n",
        "    def __init__(self, chin, chout, kernel_size):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chin),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chin),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mup = nn.MaxUnpool2d(kernel_size=2)\n",
        "\n",
        "    def forward(self, x, indices, output_size):\n",
        "        y = self.mup(x, indices, output_size=output_size)\n",
        "        y = self.seq(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ImageSegmentation(torch.nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.out_channels = 3\n",
        "        self.bn_input = nn.BatchNorm2d(3)\n",
        "        self.dc1 = DownConv2(3, 64, kernel_size=kernel_size)\n",
        "        self.dc2 = DownConv2(64, 128, kernel_size=kernel_size)\n",
        "        self.dc3 = DownConv3(128, 256, kernel_size=kernel_size)\n",
        "        self.dc4 = DownConv3(256, 512, kernel_size=kernel_size)\n",
        "        # self.dc5 = DownConv3(512, 512, kernel_size=kernel_size)\n",
        "\n",
        "        # self.uc5 = UpConv3(512, 512, kernel_size=kernel_size)\n",
        "        self.uc4 = UpConv3(512, 256, kernel_size=kernel_size)\n",
        "        self.uc3 = UpConv3(256, 128, kernel_size=kernel_size)\n",
        "        self.uc2 = UpConv2(128, 64, kernel_size=kernel_size)\n",
        "        self.uc1 = UpConv2(64, 3, kernel_size=kernel_size)\n",
        "\n",
        "    def forward(self, batch: torch.Tensor):\n",
        "        x = self.bn_input(batch)\n",
        "        # x = batch\n",
        "        # SegNet Encoder\n",
        "        x, mp1_indices, shape1 = self.dc1(x)\n",
        "        x, mp2_indices, shape2 = self.dc2(x)\n",
        "        x, mp3_indices, shape3 = self.dc3(x)\n",
        "        x, mp4_indices, shape4 = self.dc4(x)\n",
        "        # Our images are 128x128 in dimension. If we run 4 max pooling\n",
        "        # operations, we are down to 128/16 = 8x8 activations. If we\n",
        "        # do another down convolution, we'll be at 4x4 and at that point\n",
        "        # in time, we may lose too much spatial information as a result\n",
        "        # of the MaxPooling operation, so we stop at 4 down conv\n",
        "        # operations.\n",
        "        # x, mp5_indices, shape5 = self.dc5(x)\n",
        "\n",
        "        # SegNet Decoder\n",
        "        # x = self.uc5(x, mp5_indices, output_size=shape5)\n",
        "        x = self.uc4(x, mp4_indices, output_size=shape4)\n",
        "        x = self.uc3(x, mp3_indices, output_size=shape3)\n",
        "        x = self.uc2(x, mp2_indices, output_size=shape2)\n",
        "        x = self.uc1(x, mp1_indices, output_size=shape1)\n",
        "\n",
        "        return x\n",
        "    # end def\n",
        "# end class"
      ],
      "metadata": {
        "id": "BdWouIla4CHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model once on a single input batch to make sure that the model\n",
        "# runs as expected and returns a tensor with the expected shape.\n",
        "m = ImageSegmentation(kernel_size=3)\n",
        "m.eval()\n",
        "to_device(m)\n",
        "m(to_device(train_pets_inputs)).shape"
      ],
      "metadata": {
        "id": "X8ukUElm4CLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom IoU Metric for validating the model.\n",
        "def IoUMetric(pred, gt, softmax=False):\n",
        "    # Run softmax if input is logits.\n",
        "    if softmax is True:\n",
        "        pred = nn.Softmax(dim=1)(pred)\n",
        "    # end if\n",
        "\n",
        "    # Add the one-hot encoded masks for all 3 output channels\n",
        "    # (for all the classes) to a tensor named 'gt' (ground truth).\n",
        "    gt = torch.cat([ (gt == i) for i in range(3) ], dim=1)\n",
        "    # print(f\"[2] Pred shape: {pred.shape}, gt shape: {gt.shape}\")\n",
        "\n",
        "    intersection = gt * pred\n",
        "    union = gt + pred - intersection\n",
        "\n",
        "    # Compute the sum over all the dimensions except for the batch dimension.\n",
        "    iou = (intersection.sum(dim=(1, 2, 3)) + 0.001) / (union.sum(dim=(1, 2, 3)) + 0.001)\n",
        "\n",
        "    # Compute the mean over the batch dimension.\n",
        "    return iou.mean()\n",
        "\n",
        "class IoULoss(nn.Module):\n",
        "    def __init__(self, softmax=False):\n",
        "        super().__init__()\n",
        "        self.softmax = softmax\n",
        "\n",
        "    # pred => Predictions (logits, B, 3, H, W)\n",
        "    # gt => Ground Truth Labales (B, 1, H, W)\n",
        "    def forward(self, pred, gt):\n",
        "        # return 1.0 - IoUMetric(pred, gt, self.softmax)\n",
        "        # Compute the negative log loss for stable training.\n",
        "        return -(IoUMetric(pred, gt, self.softmax).log())\n",
        "    # end def\n",
        "# end class\n",
        "\n",
        "def test_custom_iou_loss():\n",
        "    #               B, C, H, W\n",
        "    x = torch.rand((2, 3, 2, 2), requires_grad=True)\n",
        "    y = torch.randint(0, 3, (2, 1, 2, 2), dtype=torch.long)\n",
        "    z = IoULoss(softmax=True)(x, y)\n",
        "    return z\n",
        "# end def\n",
        "\n",
        "test_custom_iou_loss()"
      ],
      "metadata": {
        "id": "na8VdN184CQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for a single epoch\n",
        "def train_model(model, loader, optimizer):\n",
        "    to_device(model.train())\n",
        "    cel = True\n",
        "    if cel:\n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "    else:\n",
        "        criterion = IoULoss(softmax=True)\n",
        "    # end if\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_samples = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(loader, 0):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = to_device(inputs)\n",
        "        targets = to_device(targets)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # The ground truth labels have a channel dimension (NCHW).\n",
        "        # We need to remove it before passing it into\n",
        "        # CrossEntropyLoss so that it has shape (NHW) and each element\n",
        "        # is a value representing the class of the pixel.\n",
        "        if cel:\n",
        "            targets = targets.squeeze(dim=1)\n",
        "        # end if\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_samples += targets.size(0)\n",
        "        running_loss += loss.item()\n",
        "    # end for\n",
        "\n",
        "    print(\"Trained {} samples, Loss: {:.4f}\".format(\n",
        "        running_samples,\n",
        "        running_loss / (batch_idx+1),\n",
        "    ))\n",
        "# end def\n"
      ],
      "metadata": {
        "id": "bTS6ugMX4CTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_accuracy(ground_truth_labels, predicted_labels):\n",
        "    eq = ground_truth_labels == predicted_labels\n",
        "    return eq.sum().item() / predicted_labels.numel()\n",
        "\n",
        "def print_test_dataset_masks(model, test_pets_targets, test_pets_labels, epoch, save_path, show_plot):\n",
        "    to_device(model.eval())\n",
        "    predictions = model(to_device(test_pets_targets))\n",
        "    test_pets_labels = to_device(test_pets_labels)\n",
        "    # print(\"Predictions Shape: {}\".format(predictions.shape))\n",
        "    pred = nn.Softmax(dim=1)(predictions)\n",
        "\n",
        "    pred_labels = pred.argmax(dim=1)\n",
        "    # Add a value 1 dimension at dim=1\n",
        "    pred_labels = pred_labels.unsqueeze(1)\n",
        "    # print(\"pred_labels.shape: {}\".format(pred_labels.shape))\n",
        "    pred_mask = pred_labels.to(torch.float)\n",
        "\n",
        "    # accuracy = prediction_accuracy(test_pets_labels, pred_labels)\n",
        "    iou = to_device(TM.classification.MulticlassJaccardIndex(3, average='micro', ignore_index=TrimapClasses.BACKGROUND))\n",
        "    iou_accuracy = iou(pred_mask, test_pets_labels)\n",
        "    pixel_metric = to_device(TM.classification.MulticlassAccuracy(3, average='micro'))\n",
        "    pixel_accuracy = pixel_metric(pred_labels, test_pets_labels)\n",
        "    custom_iou = IoUMetric(pred, test_pets_labels)\n",
        "    title = f'Epoch: {epoch:02d}, Accuracy[Pixel: {pixel_accuracy:.4f}, IoU: {iou_accuracy:.4f}, Custom IoU: {custom_iou:.4f}]'\n",
        "    print(title)\n",
        "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Close all previously open figures.\n",
        "    close_figures()\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 12))\n",
        "    fig.suptitle(title, fontsize=12)\n",
        "\n",
        "    fig.add_subplot(3, 1, 1)\n",
        "    plt.imshow(t2img(torchvision.utils.make_grid(test_pets_targets, nrow=7)))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Targets\")\n",
        "\n",
        "    fig.add_subplot(3, 1, 2)\n",
        "    plt.imshow(t2img(torchvision.utils.make_grid(test_pets_labels.float() / 2.0, nrow=7)))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Ground Truth Labels\")\n",
        "\n",
        "    fig.add_subplot(3, 1, 3)\n",
        "    plt.imshow(t2img(torchvision.utils.make_grid(pred_mask / 2.0, nrow=7)))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Predicted Labels\")\n",
        "\n",
        "    if save_path is not None:\n",
        "        plt.savefig(os.path.join(save_path, f\"epoch_{epoch:02}.png\"), format=\"png\", bbox_inches=\"tight\", pad_inches=0.4)\n",
        "    # end if\n",
        "\n",
        "    if show_plot is False:\n",
        "        close_figures()\n",
        "    else:\n",
        "        plt.show()\n",
        "    # end if\n",
        "# end def"
      ],
      "metadata": {
        "id": "gVqIGfAm4CVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dataset_accuracy(model, loader):\n",
        "    to_device(model.eval())\n",
        "    iou = to_device(TM.classification.MulticlassJaccardIndex(3, average='micro', ignore_index=TrimapClasses.BACKGROUND))\n",
        "    pixel_metric = to_device(TM.classification.MulticlassAccuracy(3, average='micro'))\n",
        "\n",
        "    iou_accuracies = []\n",
        "    pixel_accuracies = []\n",
        "    custom_iou_accuracies = []\n",
        "\n",
        "    print_model_parameters(model)\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(loader, 0):\n",
        "        inputs = to_device(inputs)\n",
        "        targets = to_device(targets)\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        pred_probabilities = nn.Softmax(dim=1)(predictions)\n",
        "        pred_labels = predictions.argmax(dim=1)\n",
        "\n",
        "        # Add a value 1 dimension at dim=1\n",
        "        pred_labels = pred_labels.unsqueeze(1)\n",
        "        # print(\"pred_labels.shape: {}\".format(pred_labels.shape))\n",
        "        pred_mask = pred_labels.to(torch.float)\n",
        "\n",
        "        iou_accuracy = iou(pred_mask, targets)\n",
        "        # pixel_accuracy = pixel_metric(pred_mask, targets)\n",
        "        pixel_accuracy = pixel_metric(pred_labels, targets)\n",
        "        custom_iou = IoUMetric(pred_probabilities, targets)\n",
        "        iou_accuracies.append(iou_accuracy.item())\n",
        "        pixel_accuracies.append(pixel_accuracy.item())\n",
        "        custom_iou_accuracies.append(custom_iou.item())\n",
        "\n",
        "        del inputs\n",
        "        del targets\n",
        "        del predictions\n",
        "    # end for\n",
        "\n",
        "    iou_tensor = torch.FloatTensor(iou_accuracies)\n",
        "    pixel_tensor = torch.FloatTensor(pixel_accuracies)\n",
        "    custom_iou_tensor = torch.FloatTensor(custom_iou_accuracies)\n",
        "\n",
        "    print(\"Test Dataset Accuracy\")\n",
        "    print(f\"Pixel Accuracy: {pixel_tensor.mean():.4f}, IoU Accuracy: {iou_tensor.mean():.4f}, Custom IoU Accuracy: {custom_iou_tensor.mean():.4f}\")"
      ],
      "metadata": {
        "id": "-6vA8XQ14CY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if our helper functions work as expected and if the image\n",
        "# is generated as expected.\n",
        "save_path = os.path.join(working_dir, \"segnet_basic_training_progress_images\")\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "print_test_dataset_masks(m, test_pets_inputs, test_pets_targets, epoch=0, save_path=None, show_plot=True)"
      ],
      "metadata": {
        "id": "yR8I3JLg4CcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and Learning Rate Scheduler.\n",
        "to_device(m)\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.7)"
      ],
      "metadata": {
        "id": "2EtAt1244CeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training loop. This will train the model for multiple epochs.\n",
        "#\n",
        "# epochs: A tuple containing the start epoch (inclusive) and end epoch (exclusive).\n",
        "#         The model is trained for [epoch[0] .. epoch[1]) epochs.\n",
        "#\n",
        "def train_loop(model, loader, test_data, epochs, optimizer, scheduler, save_path):\n",
        "    test_inputs, test_targets = test_data\n",
        "    epoch_i, epoch_j = epochs\n",
        "    for i in range(epoch_i, epoch_j):\n",
        "        epoch = i\n",
        "        print(f\"Epoch: {i:02d}, Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
        "        train_model(model, loader, optimizer)\n",
        "        with torch.inference_mode():\n",
        "            # Display the plt in the final training epoch.\n",
        "            print_test_dataset_masks(model, test_inputs, test_targets, epoch=epoch, save_path=save_path, show_plot=(epoch == epoch_j-1))\n",
        "        # end with\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        # end if\n",
        "        print(\"\")\n",
        "    # end for\n",
        "# end def"
      ],
      "metadata": {
        "id": "NmSCMusU4cVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train our model for 20 epochs, and record the following:\n",
        "#\n",
        "# 1. Training Loss\n",
        "# 2. Test accuracy metrics for a single batch (21 images) of test images. The following\n",
        "#    metrics are computed:\n",
        "#   2.1. Pixel Accuracy\n",
        "#   2.2. IoU Accuracy (weighted)\n",
        "#   2.3. Custom IoU Accuracy\n",
        "#\n",
        "# We also plot the following for each of the 21 images in the validation batch:\n",
        "# 1. Input image\n",
        "# 2. Ground truth segmentation mask\n",
        "# 3. Predicted segmentation mask\n",
        "#\n",
        "# so that we can visually inspect the model's progres and determine how well the model\n",
        "# is doing qualitatively. Note that the validation metrics on the set of 21 images in\n",
        "# the validation set is displayed inline in the notebook only for the last training\n",
        "# epoch.\n",
        "#\n",
        "save_path = os.path.join(working_dir, \"segnet_basic_training_progress_images\")\n",
        "train_loop(m, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "WdKBhppC4ccY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's checkpoint.\n",
        "save_model_checkpoint(m, f\"pets_segnet_CrossEntropyLoss_LRSchedule_20_epochs.pth\")"
      ],
      "metadata": {
        "id": "eUT6nbRw4cgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_parameters(m)"
      ],
      "metadata": {
        "id": "oAlg4w6q4cke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition. We use a SegNet-Basic model with some minor tweaks.\n",
        "# Our input images are 128x128.\n",
        "# In this model, we use depth-wise-separable convolutions instead of\n",
        "# \"regular\" convolutions.\n",
        "\n",
        "class DepthwiseSeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, bias=True):\n",
        "        super().__init__()\n",
        "        # The depthwise conv is basically just a grouped convolution in PyTorch with\n",
        "        # the number of distinct groups being the same as the number of input (and output)\n",
        "        # channels for that layer.\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, bias=bias, groups=in_channels)\n",
        "        # The pointwise convolution stretches across all the output channels using\n",
        "        # a 1x1 kernel.\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class DownDSConv2(nn.Module):\n",
        "    def __init__(self, chin, chout, kernel_size):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mp = nn.MaxPool2d(kernel_size=2, return_indices=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.seq(x)\n",
        "        pool_shape = y.shape\n",
        "        y, indices = self.mp(y)\n",
        "        return y, indices, pool_shape\n",
        "\n",
        "class DownDSConv3(nn.Module):\n",
        "    def __init__(self, chin, chout, kernel_size):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv2d(in_channels=chout, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mp = nn.MaxPool2d(kernel_size=2, return_indices=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.seq(x)\n",
        "        pool_shape = y.shape\n",
        "        y, indices = self.mp(y)\n",
        "        return y, indices, pool_shape\n",
        "\n",
        "class UpDSConv2(nn.Module):\n",
        "    def __init__(self, chin, chout, kernel_size):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chin),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mup = nn.MaxUnpool2d(kernel_size=2)\n",
        "\n",
        "    def forward(self, x, indices, output_size):\n",
        "        y = self.mup(x, indices, output_size=output_size)\n",
        "        y = self.seq(y)\n",
        "        return y\n",
        "\n",
        "class UpDSConv3(nn.Module):\n",
        "    def __init__(self, chin, chout, kernel_size):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chin),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chin, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chin),\n",
        "            nn.ReLU(),\n",
        "            DepthwiseSeparableConv2d(in_channels=chin, out_channels=chout, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(chout),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mup = nn.MaxUnpool2d(kernel_size=2)\n",
        "\n",
        "    def forward(self, x, indices, output_size):\n",
        "        y = self.mup(x, indices, output_size=output_size)\n",
        "        y = self.seq(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ImageSegmentationDSC(torch.nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.out_channels = 3\n",
        "        self.bn_input = nn.BatchNorm2d(3)\n",
        "        self.dc1 = DownDSConv2(3, 64, kernel_size=kernel_size)\n",
        "        self.dc2 = DownDSConv2(64, 128, kernel_size=kernel_size)\n",
        "        self.dc3 = DownDSConv3(128, 256, kernel_size=kernel_size)\n",
        "        self.dc4 = DownDSConv3(256, 512, kernel_size=kernel_size)\n",
        "        # self.dc5 = DownConv3(512, 512, kernel_size=kernel_size)\n",
        "\n",
        "        # self.uc5 = UpConv3(512, 512, kernel_size=kernel_size)\n",
        "        self.uc4 = UpDSConv3(512, 256, kernel_size=kernel_size)\n",
        "        self.uc3 = UpDSConv3(256, 128, kernel_size=kernel_size)\n",
        "        self.uc2 = UpDSConv2(128, 64, kernel_size=kernel_size)\n",
        "        self.uc1 = UpDSConv2(64, 3, kernel_size=kernel_size)\n",
        "\n",
        "    def forward(self, batch: torch.Tensor):\n",
        "        x = self.bn_input(batch)\n",
        "        # x = batch\n",
        "        # SegNet Encoder\n",
        "        x, mp1_indices, shape1 = self.dc1(x)\n",
        "        x, mp2_indices, shape2 = self.dc2(x)\n",
        "        x, mp3_indices, shape3 = self.dc3(x)\n",
        "        x, mp4_indices, shape4 = self.dc4(x)\n",
        "        # Our images are 128x128 in dimension. If we run 4 max pooling\n",
        "        # operations, we are down to 128/16 = 8x8 activations. If we\n",
        "        # do another down convolution, we'll be at 4x4 and at that point\n",
        "        # in time, we may lose too much spatial information as a result\n",
        "        # of the MaxPooling operation, so we stop at 4 down conv\n",
        "        # operations.\n",
        "        # x, mp5_indices, shape5 = self.dc5(x)\n",
        "\n",
        "        # SegNet Decoder\n",
        "        # x = self.uc5(x, mp5_indices, output_size=shape5)\n",
        "        x = self.uc4(x, mp4_indices, output_size=shape4)\n",
        "        x = self.uc3(x, mp3_indices, output_size=shape3)\n",
        "        x = self.uc2(x, mp2_indices, output_size=shape2)\n",
        "        x = self.uc1(x, mp1_indices, output_size=shape1)\n",
        "\n",
        "        return x\n",
        "    # end def\n",
        "# end class"
      ],
      "metadata": {
        "id": "TnEBa-3U4cnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageSegmentationWithScattering(torch.nn.Module):\n",
        "    def __init__(self, kernel_size, scattering):\n",
        "        super().__init__()\n",
        "        self.scattering = scattering\n",
        "        print(kernel_size)\n",
        "\n",
        "        num_channels_after_scattering = 3 * 81\n",
        "\n",
        "        self.bn_input = nn.BatchNorm2d(num_channels_after_scattering)\n",
        "        self.dc1 = DownConv2(num_channels_after_scattering, 64, kernel_size=kernel_size)\n",
        "        self.dc2 = DownConv2(64, 128, kernel_size=kernel_size)\n",
        "        self.dc3 = DownConv3(128, 256, kernel_size=kernel_size)\n",
        "        self.dc4 = DownConv3(256, 512, kernel_size=kernel_size)\n",
        "\n",
        "        self.uc4 = UpConv3(512, 256, kernel_size=kernel_size)\n",
        "        self.uc3 = UpConv3(256, 128, kernel_size=kernel_size)\n",
        "        self.uc2 = UpConv2(128, 64, kernel_size=kernel_size)\n",
        "        self.uc1 = UpConv2(64, 3, kernel_size=kernel_size)\n",
        "        self.uc0 = nn.ConvTranspose2d(3, 3, kernel_size=2**scattering.J, dilation = scattering.J-1, stride = 2**scattering.J)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape0 = x.shape\n",
        "        x = self.scattering(x)\n",
        "        x = x.view(x.size(0), -1, x.size(3), x.size(4))\n",
        "        x = self.bn_input(x)\n",
        "        x, mp1_indices, shape1 = self.dc1(x)\n",
        "        x, mp2_indices, shape2 = self.dc2(x)\n",
        "        x, mp3_indices, shape3 = self.dc3(x)\n",
        "        x, mp4_indices, shape4 = self.dc4(x)\n",
        "\n",
        "        x = self.uc4(x, mp4_indices, output_size=shape4)\n",
        "        x = self.uc3(x, mp3_indices, output_size=shape3)\n",
        "        x = self.uc2(x, mp2_indices, output_size=shape2)\n",
        "        x = self.uc1(x, mp1_indices, output_size=shape1)\n",
        "        x = self.uc0(x, output_size=shape0)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "scattering_layer = Scattering2D(J=2, shape=(128, 128))\n",
        "model_with_scattering = ImageSegmentationWithScattering(kernel_size=3, scattering=scattering_layer)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_with_scattering.to(device)\n"
      ],
      "metadata": {
        "id": "tg6nrbtR4cqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and Learning Rate Scheduler.\n",
        "# to_device(m)\n",
        "pets_input_grid = torchvision.utils.make_grid(train_pets_inputs, nrow=8)\n",
        "pets_targets_grid = torchvision.utils.make_grid(train_pets_targets / 2.0, nrow=8)\n",
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.7)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "4iJhKNBu4xFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_test_dataset_masks(model_with_scattering, test_pets_inputs, test_pets_targets, epoch=0, save_path=None, show_plot=True)"
      ],
      "metadata": {
        "id": "ueRYE3fp4xHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import itertools\n",
        "\n",
        "# # Define the hyperparameters to tune\n",
        "# hyperparameters = {\n",
        "#     'J': [1, 2, 3],  # Number of scales\n",
        "#     'input_size': [(128, 128), (256, 256)],  # Input image size\n",
        "#     'depth': [2, 3, 4],  # Depth of the network\n",
        "#     'dropout_rate': [0.0, 0.1, 0.2],  # Dropout rate\n",
        "#     'learning_rate': [0.001, 0.01, 0.1],  # Learning rate\n",
        "# }\n",
        "\n",
        "# # Generate all possible combinations of hyperparameters\n",
        "# param_combinations = list(itertools.product(*hyperparameters.values()))\n",
        "\n",
        "# # Define a function to train and evaluate the model with a given set of hyperparameters\n",
        "# def train_and_evaluate(params):\n",
        "#     J, input_size, depth, dropout_rate, learning_rate = params\n",
        "\n",
        "#     # Create the scattering layer with the specified hyperparameters\n",
        "#     scattering_layer = Scattering2D(J=J, shape=input_size)\n",
        "\n",
        "#     # Assuming 'ImageSegmentationWithScattering' is a custom model class you have defined that integrates the scattering layer\n",
        "#     model_with_scattering = ImageSegmentationWithScattering(kernel_size=3, scattering=scattering_layer)\n",
        "\n",
        "#     # Move the model to the appropriate device (e.g., GPU)\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model_with_scattering.to(device)\n",
        "\n",
        "#     # Define optimizer and scheduler with specified learning rate\n",
        "#     optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=learning_rate)\n",
        "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.7)\n",
        "\n",
        "#     # Assuming 'train_loop', 'test_dataset_accuracy', 'pets_train_loader', 'pets_test_loader' are defined elsewhere in your code\n",
        "#     train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)\n",
        "\n",
        "#     # Evaluate the model on the test dataset\n",
        "#     accuracy_metrics = test_dataset_accuracy(model_with_scattering, pets_test_loader)\n",
        "\n",
        "#     return accuracy_metrics\n",
        "\n",
        "# # Iterate over all parameter combinations and train/evaluate the model\n",
        "# results = {}\n",
        "# for params in param_combinations:\n",
        "#     print(f\"Training model with hyperparameters: {params}\")\n",
        "#     metrics = train_and_evaluate(params)\n",
        "#     results[params] = metrics\n",
        "\n",
        "# # Print the results\n",
        "# for params, metrics in results.items():\n",
        "#     print(f\"Hyperparameters: {params}, Metrics: {metrics}\")"
      ],
      "metadata": {
        "id": "UByYY5ff4xKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pets_input_grid = torchvision.utils.make_grid(train_pets_inputs, nrow=8)\n",
        "pets_targets_grid = torchvision.utils.make_grid(train_pets_targets / 2.0, nrow=8)\n",
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "i030iHcb4xOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "k3z38V_P4xPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.02)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "svGYCINc47Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.85)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "9w8PsWac47Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.015)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.75)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "zgu1OODl47XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.007)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.65)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "AoRsS7b647Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.03)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "kuRFWep447cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.008)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.7)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "3tQznGJ147e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.012)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "YfCqRBpW47iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.009)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=11, gamma=0.8)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "xxMDPRDa47lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.006)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.77)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "6ToRzTr347mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.85)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "c3aGGfvb47nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.002)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.95)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "qgQC5goc47rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "VOEiwtUo5QUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.013)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.9)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "8XraiSU25QX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.011)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.6)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "lUfAMWHI5QZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.004)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.75)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "o7LWJojp5Qb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.014)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.82)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "Bm8S5g775Qep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.007)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.88)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "o7nyhXFJ5Qie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_with_scattering.parameters(), lr=0.018)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.7)\n",
        "\n",
        "train_loop(model_with_scattering, pets_train_loader, (test_pets_inputs, test_pets_targets), (1, 21), optimizer, scheduler, save_path)"
      ],
      "metadata": {
        "id": "DaSWsnsi5bpb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}